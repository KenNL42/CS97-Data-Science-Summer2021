{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day8_Perceptron_NN_Discussion.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMu3ilsW/BJEVOnxYTBUDE7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dJA8UafOcC9q"},"source":["#Perceptron and Neural Network (Day8 7/21)"]},{"cell_type":"markdown","metadata":{"id":"5gUK4scNcgf1"},"source":["##Agenda\n","\n","\n","1. Anatomy of perceptron\n","2. Perceptron example\n","3. Anatomy of Neural Network\n","4. Math behind Neural Network and backpropagation\n","5. Neural Network Playground\n","6. Extras"]},{"cell_type":"markdown","metadata":{"id":"bYWTnIXdc4Ww"},"source":["#Anatomy of perceptron\n","\n","##Definition: \n","Perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class (Wikipedia).\n","\n","##Background:\n","\n","![neuron_img](https://www.brainfacts.org/-/media/Brainfacts2/For-Educators/For-the-Classroom/Light-Up-Neuron/neuron_labels_c-replacement.png)\n","\n","Perceptron’s goal is to mimic how a neuron in our brain works.\n","\n","Neurons receive signal from cell body, and it propagates signals along axon. The output signals are “all or nothing” (can be simplified as either 0 or 1). In terms of ML, we would like to compute whether an output is 0 or 1, given inputs.\n","\n","##Motivation:\n","![perceptron_graph](https://miro.medium.com/max/1400/1*xsR57_PO8U7PB_ItLslLmA.png)\n","\n","Algebraically, perceptron classifies data by adjusting its weight and bias, then data in one side of the region will be classified as a particular class. This is similar to line algebra where you have slope and y-intercept.\n","\n","##Perceptron Components: \n","A perceptron has:\n","-\tInput values\n","-\tWeight\n","-\tBias\n","-\tActivation function\n","\n","![perceptron_img](https://miro.medium.com/max/645/0*LJBO8UbtzK_SKMog)\n","\n","*Note: the picture does not have activation function and bias*\n","\n","To compute for an output for a single perceptron, we calculate:\n","\n","$output = \\sigma(\\sum_{i=1}^n w_i x_i$)\n","\n","where \n","- $\\sigma$ is an activation function\n","- $x_i$ is an input\n","- $w_i$ is a weight for $x_i$\n","- $b_i$ is bias for $x_i$\n","\n","Notice how we need to specify activation function. In ML field, there is no right or wrong answer on what should be the ideal activation function. Here are some examples:\n","![activation_img](https://miro.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png)\n","\n","In general, we commonly use ReLU because it is simple and fast."]},{"cell_type":"markdown","metadata":{"id":"fHQSQ36ygUsa"},"source":["#Perceptron Example\n","\n","##Example: \n","Let’s say you want to have lunch. You consider these four factors:\n","-\tIs there anything left in the fridge? (0 as none, 1 as there are food left)\n","-\tIs there any restaurant within 3 miles? (0 as none, 1 as there are restaurants)\n","-\tIs it sunny now? (0 as raining, 1 as clear sky)\n","-\tAre you using RTX3090? (0 as no, 1 as yes)\n","![perceptron_example](https://drive.google.com/uc?export=view&id=1kkYCUNg75xvdLFRClRrtcrBgBwXRRfj9)\n","\n","We want to use these four factors to decide whether you want to eat a leftover food or you want to get a lunch from a restaurant. Let’s classify if $ output <=0$, you want to eat leftover food, else you want to grab food from outside. For simplicity, let’s set every bias to zero and use ReLU as an activation function.\n","\n","We let \n","\n","$$\n","    ReLU(x) =\n","    \\begin{cases}\n","        0 & \\text{if $x<=0$,}\\\\\n","        x & \\text{if $x>0$}\n","    \\end{cases}\n","$$\n","\n","In practice, we initialize weights and biases with random number, and we use backpropagation algorithm (will discuss later) to adjust the weights and biases correctly. For this problem, let us initialize the weight analytically.\n","\n","Imagine a weight as “how important is this feature?” If we want to emphasize that a feature is very important, we then adjust its weight to some large positive number (or negative if it inversely affects the importance). Otherwise, if a feature is less important, we then adjust its weight to some small number.\n","\n","![perceptron_calculated](https://drive.google.com/uc?export=view&id=1srvS1VJpLc4XXDogbX2Qj4DVXckEPVUN)\n","\n","From earlier section, we let $\\sigma = ReLU(Z)$ as our activation function.\n","\n","We then substitute $Z$ with $Z = \\sum_{i=1}^4 w_i x_i$\n","\n","We calculate \n","\n","$$\n","\\begin{align}\n","Z & = \\sum_{i=1}^4 w_i x_i \\\\\n","& = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 \\\\\n","& = (1\\cdot-3)+(1\\cdot2)+(0\\cdot3)+(1\\cdot0.01) \\\\\n","& = -3 + 2 + 0 + 0.01 \\\\\n","& = -0.99\n","\\end{align}\n","$$\n","\n","And we then compute output \n","\n","$$\n","\\begin{align}\n","output & = \\sigma(Z) \\\\\n","& = ReLU(Z) \\\\\n","& = ReLU(-0.99) \\\\ \n","& = 0\n","\\end{align}\n","$$\n","\n","Suppose you believe having leftover food in the fridge is important, then you initialize its weight to -3. Having RTX3090 has less effect to how you decide to eat, so you initialize its weight to 0.01. After final calculation with activation function, it returns 0. So you decide to not eat outside and eat the food you have."]},{"cell_type":"markdown","metadata":{"id":"QPWEiEn_lGwN"},"source":["#Neural Network\n","\n","##Long Definition (optional): \n","Artificial neural networks, usually simply called neural networks (NNs), are based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it.\n","\n","##Short explanation: \n","Neural Network is a bunch of perceptron connected together\n","\n","##Motivation:\n","The perceptron is not complex enough to classify non-linear data. We then introduced neural network which consists of many perceptrons to handle non-linear classification problem.\n","\n","Link: https://playground.tensorflow.org/\n","\n","![playground](http://i.imgur.com/rbB43iO.png?1)\n","\n","You can try to decrease hidden layer to 1 neuron only, which is equivalent to a perceptron. It classifies data poorly. But after you introduces more perceptron by increasing neurons, the model works much better.\n","\n","##Definition & Terminology: \n","A neural network consists of layers, and each layer consists of nodes. Each connection between two nodes is a representation of parameters, which are weight and bias. For layers, there are input layer, hidden layer, and output layer."]},{"cell_type":"markdown","metadata":{"id":"L66XwMxeop9D"},"source":["#Backpropagation (Math example)\n","\n","##Backpropagation in 24 words: We are given the cost function. We use the cost function to update each weight and bias back from output layer to input layer.\n","\n","In earlier perceptron example, we already initialized weights and biases. In practice, those are initially randomized and then update by using gradient descent.\n","\n","Let's look at an example of how backpropagation works after we first initialize every weight and bias (credit to [3Blue1Brown's video](https://www.youtube.com/watch?v=tIeHLnjs5U8) and [StatQuest's video](https://www.youtube.com/watch?v=IN2XmBhILt4))\n","\n","Suppose our neural network looks like this:\n","![nn](https://drive.google.com/uc?export=view&id=14UjQj7wp0QbvPabr-BBwgnItug0KIBhd)\n","\n","Now, let's focus on only last two nodes:\n","![nn2](https://drive.google.com/uc?export=view&id=18a2qC6awdBEokR9oid8lpsG0gzLj9Mtb)\n","\n","We know that our cost function for one data is\n","\n","$\n","Cost(\\theta) = (Predicted - Observed)^2 = (activation^{(L)} - Observed)^2\n","$\n","\n","where $\\theta$ consists of parameters. In this case, our parameters are $w _1$, $b_1$, $w_2$, $b_2$, $w_3$, $b_3$.\n","\n","Note: for this notation, the superscript $L$ in $activation^{(L)}$ tells us we are at hidden layer $L$. In this case, we are in activation function of output layer\n","\n","##Let us define more formulas. \n","\n","Let\n","$Z^{(L)} = w^{(L)} \\cdot activation^{(L-1)} + b^{(L)}$ which is an output of one perceptron\n","\n","Let\n","$activation^{(L)} = \\sigma(Z^{(L)})$ which is an activation function for an output of perceptron\n","\n","##Analysis\n","We can see that each activation function consists of weight, bias, and activation output from the last layer\n","\n","![nn3](https://drive.google.com/uc?export=view&id=1cIigRioxMgmxFIH32_rfOr_6wX4eM8ie)\n","\n","##Use Chain Rule to update each variable\n","We will use chain rule to calculate how much to change $w_3$ given that we have the cost function value. **Note: in simplified word, chain rule is a way to see how much one variable change when we change another variable.** There is a proof note under \"Extra\" section\n","\n","In this case, let's assume that we already find true value of everything except $w_3$ at the last node. We then find a way to update $w_3$ by using cost function. **Note: you don't need to understand calculus behind the proof. *Again, tl;dr -> Backpropagation in 24 words: We are given the cost function. We use the cost function to update each weight and bias back from output layer to input layer.***\n","\n","We will have $\\frac{\\partial Cost}{\\partial w_3}$ by using chain rule.\n","After that, we update $w_3$ by gradient descent\n","\\begin{equation}\n","w_3^{(new)} = w_3^{(old)} - learningrate \\cdot \\frac{\\partial Cost}{\\partial w_3}\n","\\end{equation}\n","\n","And after each update for each variable, we will use it to update former parameters from prior layer. Then we repeatedly train the model and use backpropagation to update variables until the model output converges."]},{"cell_type":"markdown","metadata":{"id":"PJwVxANVHGk0"},"source":["#Neural Network Code Example"]},{"cell_type":"markdown","metadata":{"id":"iaokEESrLsuY"},"source":["Credit: adapted from Crashcourse AI (https://www.youtube.com/watch?v=6nGCGYWMObE)"]},{"cell_type":"code","metadata":{"id":"nm0kEprgIK6s"},"source":["from keras.datasets import mnist\n","import matplotlib.pyplot as plt\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","\n","img_index = 1 # <-- change this index to see different number picture\n","\n","print(\"Number: {}\".format(y_train[img_index]))\n","plt.imshow(X_train[img_index], cmap=plt.get_cmap('gray'))\n","# show the figure\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H7HigWFcKUsp"},"source":["# Convert pixel range to between 0 and 1 to make the model learns easier\n","X_train = X_train / 255\n","X_test = X_test / 255\n","\n","# convert all 2D pixel picture into 1D vector\n","X_train = X_train.reshape(60000,784)\n","X_test = X_test.reshape(10000,784)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSFua67hHPQ1"},"source":["from sklearn.neural_network import MLPClassifier # import multi-layer perceptron classifier\n","\n","hidden_layer_architecture = (100, 100) # <----- try changing the hidden layer to make the model more accurate\n","\n","mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_architecture, max_iter=10, alpha=1e-4, \n","                     solver='sgd', verbose=10, tol=1e-4, random_state=1,\n","                     learning_rate_init=.1)\n","mlp.fit(X_train, y_train)\n","print('\\ntraining score: {}'.format(mlp.score(X_train, y_train)))\n","print('test score: {}'.format(mlp.score(X_test, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GODeTElcMB_8"},"source":["predicted_index = 1954 # <--- change this index to see different test data\n","\n","output = mlp.predict([X_test[predicted_index]])\n","print(\"Predict below picture as: {} \\nwhile the true label is: {}\".format(output[0], y_test[predicted_index]))\n","plt.imshow(X_test[predicted_index].reshape((28,28)), cmap=plt.get_cmap('gray'))\n","# show the figure\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEFqHCMU6Z1V"},"source":["#Extra:\n","\n","## Book about neural network\n","- http://neuralnetworksanddeeplearning.com/index.html\n","\n","\n","## youtube explanation for neural network\n","- https://www.youtube.com/watch?v=tIeHLnjs5U8\n","- https://www.youtube.com/watch?v=IN2XmBhILt4\n","\n","## Chain rule proof for $w_3$\n","![nn4](https://drive.google.com/uc?export=view&id=1Ni7U0LasOUGR_KuZCqUz6tShDqhufJxq)"]}]}